{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Deep Dive into Attention Mechanisms \n",
    "\n",
    "This notebook teaches and tests LLMs from Scratch Chapter 3 concepts (attention, casual masking, multi-head) with clean Pytorch. \n",
    "It builds off of chapter 2 and provides a detailed exploration of attention mechanisms in transformers. \n",
    "\n",
    "## Chapter 2 Working with Text Data (recap)\n",
    "\n",
    "* **Tokenization:** We split raw text into tokens using rules like byte‑pair encoding (BPE).  Tokenization reduces a large vocabulary into manageable units by merging common pairs of characters.\n",
    "* **Vocabulary & token IDs:** Each distinct token is assigned an integer ID.  These IDs are what we feed into our model.\n",
    "* **Embedding layer:** A lookup table that turns each token ID into a fixed‑length dense vector.  Similar words end up with similar vectors once the model is trained.\n",
    "* **Dataloader:** We prepare sequences of token IDs of a fixed length, often with overlapping windows, to feed batches into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: [' ', '.', 'H', 'T', 'a', 'd', 'e', 'f', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'z']\n",
      "Sample token IDs (first 20): [2, 6, 11, 11, 14, 0, 20, 14, 16, 11, 5, 1, 0, 3, 8, 9, 17, 0, 9, 17]\n",
      "Embeddings for first 10 tokens (shape): torch.Size([10, 8])\n",
      "tensor([[ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806],\n",
      "        [-0.9138, -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360],\n",
      "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835],\n",
      "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835],\n",
      "        [-1.4570, -0.1023, -0.5992,  0.4771,  0.7262,  0.0912, -0.3891,  0.5279],\n",
      "        [ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047],\n",
      "        [-1.9006,  0.2286,  0.0249, -0.3460,  0.2868, -0.7308,  0.1748, -1.0939],\n",
      "        [-1.4570, -0.1023, -0.5992,  0.4771,  0.7262,  0.0912, -0.3891,  0.5279],\n",
      "        [ 1.9312,  1.0119, -1.4364, -1.1299, -0.1360,  1.6354,  0.6547,  0.5760],\n",
      "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "Building a basic tokenizer.\n",
    "\"\"\"\n",
    "text = \"Hello world. This is a simple test of our tokenizer.\"\n",
    "\n",
    "# 1. Build a character vocabulary\n",
    "vocab = sorted(list(set(text)))\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# 2. Convert characters to token IDs\n",
    "ids = [stoi[ch] for ch in text]\n",
    "\n",
    "print('Vocabulary:', vocab)\n",
    "print('Sample token IDs (first 20):', ids[:20])\n",
    "\n",
    "# 3. Define embedding layer\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 8\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 4. Show how token IDs map to embeddings\n",
    "# Pick first 10 characters and look up their embeddings\n",
    "sample_ids = torch.tensor(ids[:10])\n",
    "emb_vectors = embedding(sample_ids)\n",
    "print('Embeddings for first 10 tokens (shape):', emb_vectors.shape)\n",
    "print(emb_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector: tensor([[ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Linear layer output (should match): tensor([[ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comparing Embedding Layers with Linear Layers\n",
    "embedding layer - a lookup table with shape (vocab_size, embedding_dimension). When you index it with a token ID it returns the corresponding row.\n",
    "linear layer - fully connected layer with shape (embedding_dimension, vocab_size) and it can perform the same operation \n",
    "if you feed it a one-hot vector for each token ID and transpose its weight matrix. \n",
    "\n",
    "Using a linear year directly would be much less efficient for large vocabularies. Why? \n",
    "Because one-hot vectors are mostly zeros, so the embedding layer will avoid constructing these sparse vectors and simply index into its weight matrix. \n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Create a linear layer with weight equal to the embedding weights (transposed)\n",
    "linear = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "#Copy the weights linear.weight has shape (embedding_dim, vocab_size)\n",
    "linear.weight.data = embedding.weight.data.clone().t()\n",
    "\n",
    "token_id = 3\n",
    "\n",
    "embed_vec = embedding(torch.tensor([token_id])) #shape (1, embedding_dim)\n",
    "\n",
    "#equivalent linear operation using a one-hot vector\n",
    "one_hot = F.one_hot(torch.tensor([token_id]), num_classes=vocab_size).float()\n",
    "lin_vec = linear(one_hot)\n",
    "\n",
    "print('Embedding vector:', embed_vec)\n",
    "print('Linear layer output (should match):', lin_vec)\n",
    "\n",
    "## they produce the same result but the embedding layer is just an efficient way to perform this lookup without constructing one-hot vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Model\n",
    "- Embeddings (B, T, d_model) project to Queries (Q), Keys (K), Values (V)\n",
    "- scaled dot-product attention is the softmax((Q K^T)/sqrt(d_head)) V\n",
    "- casual mask (lower-triangular) to prevent looking ahead\n",
    "- multi-head is to split channels into heads, attend in parallel, concat, and project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding matrix learns a semantic space so similar tokens are nearby.\n",
    "\n",
    "B, T, d_model = 2, 6, 48\n",
    "x = torch.randn(B, T, d_model, device=device)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Head Self-Attention & Q, K, V \n",
    "- W_Q: how to **ask** (what features to look for)\n",
    "- W_K: how to **advertise** (what features the token contains)\n",
    "- W_V: what to **carry** (content to pass along)\n",
    "\n",
    "In a transformer, self-attention allows each position in a sequence to **attend** to other positions to gather contextual information. A single head of self-attention works as follows --\n",
    "\n",
    "1. For each token vector in the input sequence, we compute three different representations (W_Q, W_K, W_V) which are learned weight matrices. \n",
    "2. Then, we compute the attention scores by taking the dot product between each query and all keys to measure similarity. \n",
    "3. Then we apply the softmax which converts the scores into a probability distrbution over the positions.\n",
    "4. Finally, we multiply the value vectors by these probabilities and sum them up. This produces a new representation for each token that incorporates contextual information from the entire (or masked) sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 16])\n",
      "Query shape: torch.Size([2, 5, 8])\n",
      "Attention weights shape: torch.Size([2, 5, 5])\n",
      "Output shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "B = 2       # batch size\n",
    "T = 5       # sequence length\n",
    "d_model = 16  # input embedding dimension\n",
    "d_head = 8    # attention head dimension\n",
    "\n",
    "# random input tensor (batch of sequences)\n",
    "x = torch.randn(B, T, d_model)\n",
    "\n",
    "# learnable projection matrices\n",
    "W_Q = torch.randn(d_model, d_head)\n",
    "W_K = torch.randn(d_model, d_head)\n",
    "W_V = torch.randn(d_model, d_head)\n",
    "\n",
    "# Compute Q, K, V\n",
    "q = x @ W_Q # shape (B, T, d_head)\n",
    "k = x @ W_K\n",
    "v = x @ W_V\n",
    "\n",
    "# Compute scaled dot-product attention\n",
    "scores = q @ k.transpose(-2, -1) / math.sqrt(d_head) # shape (B, T, T)\n",
    "weights = torch.softmax(scores, dim=-1) \n",
    "\n",
    "#weighted sum of values\n",
    "out = weights @ v\n",
    "\n",
    "\n",
    "print('Input shape:', x.shape)\n",
    "print('Query shape:', q.shape)\n",
    "print('Attention weights shape:', weights.shape)\n",
    "print('Output shape:', out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
