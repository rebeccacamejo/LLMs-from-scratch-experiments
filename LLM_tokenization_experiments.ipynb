{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd640ace",
   "metadata": {},
   "source": [
    "\n",
    "# LLM Tokenization & Dataloader Experiments\n",
    "\n",
    "This notebook contains four hands-on experiments that mirror the ideas from *LLMs From Scratch* (ch.2):\n",
    "\n",
    "1. **Regex drills:** tweak the punctuation class; run on custom text and measure token count and vocabulary size.  \n",
    "2. **Break the toy tokenizer:** show OOV behavior and add a minimal `[UNK]` path to `SimpleTokenizerV1`.  \n",
    "3. **Swap tokenizers:** rebuild the dataloader using **tiktoken (GPT‑2)** when available and compare to the regex tokenizer.  \n",
    "4. **Stride experiment:** fix `max_length`, vary stride ∈ {1, 8, 64}, plot how many samples we get and inspect overlap.\n",
    "\n",
    "Each section includes:\n",
    "- An intuitive description\n",
    "- Executable code\n",
    "- A short **Conclusion** and **Vocabulary learned**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5317366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core imports\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import tiktoken; fall back gracefully if unavailable\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    TIKTOKEN_IMPORT_ERROR = e\n",
    "\n",
    "print(\"tiktoken available:\", TIKTOKEN_AVAILABLE)\n",
    "if not TIKTOKEN_AVAILABLE:\n",
    "    print(\"Note: tiktoken not available in this environment. We'll provide a lightweight fallback tokenizer so the notebook still runs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7d83e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Regex drills — tweak the punctuation class\n",
    "**Goal:** Feel how the tokenizer design changes token counts and vocabulary size.\n",
    "\n",
    "**What you do:** Adjust a punctuation class in a regex-based tokenizer and re-run on your own paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae315642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Editable input text (write your own paragraph here) ----\n",
    "text = (\n",
    "    \"I’m building a tiny tokenizer, testing dashes—both en– and em—plus quotes, “smart quotes”, and ellipses... \"\n",
    "    \"Also: URLs like https://example.com, emails (me@example.com), and numbers 3.14159!\"\n",
    ")\n",
    "\n",
    "# Baseline punctuation class (feel free to edit)\n",
    "# The idea: tokens are either \"words\" (letters/digits/underscore/apostrophe) or single punctuation marks.\n",
    "punctuation_class = r\"[\\\\.\\\\,\\\\;\\\\:\\\\!\\\\?\\\\-\\\\—\\\\–\\\\(\\\\)\\\\[\\\\]\\\\{\\\\}\\\"\\\\'\\\\…]\"  # tweak me!\n",
    "token_pattern = rf\"{punctuation_class}|\\\\w+|\\\\S\"\n",
    "\n",
    "tokens = re.findall(token_pattern, text)\n",
    "vocab = sorted(set(tokens))\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(\"First 30 tokens:\", tokens[:30])\n",
    "print(\"Vocabulary sample:\", vocab[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fda7c1",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion:**  \n",
    "Changing the punctuation class alters how characters like dashes (–, —), quotes (“ ”), and ellipses (…) are split.  \n",
    "This directly changes both **token count** (sequence length) and **vocabulary size** (unique tokens).\n",
    "\n",
    "**Vocabulary learned:** *tokenization, regex, vocabulary, token length vs. granularity trade‑off*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b372335",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Break the toy tokenizer → add a minimal `[UNK]`\n",
    "**Goal:** See why word-level tokenizers struggle with out-of-vocabulary (OOV) tokens and how `[UNK]` patches the error (but loses information).\n",
    "\n",
    "**What you do:** Build a tiny word-level tokenizer from a corpus, then try to encode an unseen word. Add `[UNK]` handling and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a tiny vocab from a small corpus\n",
    "corpus = \"the quick brown fox jumps over the lazy dog. the dog sleeps.\"\n",
    "word_tokens = re.findall(r\"\\w+|[^\\w\\s]\", corpus.lower())\n",
    "vocab = sorted(set(word_tokens))\n",
    "stoi = {tok: i for i, tok in enumerate(vocab)}\n",
    "itos = {i: tok for tok, i in stoi.items()}\n",
    "\n",
    "def encode_strict(text):\n",
    "    toks = re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "    return [stoi[t] for t in toks]  # KeyError if unseen\n",
    "\n",
    "def encode_with_unk(text, unk_token=\"[UNK]\"):\n",
    "    # ensure UNK in vocab\n",
    "    if unk_token not in stoi:\n",
    "        idx = len(stoi)\n",
    "        stoi[unk_token] = idx\n",
    "        itos[idx] = unk_token\n",
    "    toks = re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "    ids = [stoi[t] if t in stoi else stoi[unk_token] for t in toks]\n",
    "    return ids\n",
    "\n",
    "# Try an unseen word\n",
    "test_text = \"the fox plays jazz\"  # 'plays' and 'jazz' are unseen\n",
    "print(\"Vocab:\", vocab)\n",
    "print(\"\\\\nAttempting strict encode (will error if OOV):\")\n",
    "try:\n",
    "    print(encode_strict(test_text))\n",
    "except Exception as e:\n",
    "    print(\"Strict encode error:\", repr(e))\n",
    "\n",
    "print(\"\\\\nEncoding with [UNK]:\")\n",
    "ids = encode_with_unk(test_text)\n",
    "print(ids)\n",
    "print(\"Decoded (approx):\", [itos[i] for i in ids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555abe7",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion:**  \n",
    "A strict word-level tokenizer throws errors on **OOV** tokens. Adding `[UNK]` prevents crashes but conflates many unknowns into the **same** token, losing detail. This motivates **subword** tokenizers (like BPE/byte-level) that decompose unfamiliar words into known pieces without `[UNK]`.\n",
    "\n",
    "**Vocabulary learned:** *OOV (out‑of‑vocabulary), `[UNK]`, word‑level vs. subword tokenization*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b56253",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Swap tokenizers — regex vs. `tiktoken` (GPT‑2) for a dataloader\n",
    "**Goal:** Compare sample counts and average sequence length when using a basic regex tokenizer vs. GPT‑2’s tokenizer.\n",
    "\n",
    "**What you do:**  \n",
    "- Implement a simple sliding-window dataset.  \n",
    "- Tokenize the same text with the two tokenizers.  \n",
    "- Build batches and compare: number of samples, and average sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a40e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text (feel free to paste a paragraph or short story excerpt)\n",
    "sample_text = (\n",
    "    \"Alice was beginning to get very tired of sitting by her sister on the bank, \"\n",
    "    \"and of having nothing to do: once or twice she had peeped into the book her sister was reading, \"\n",
    "    \"but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice \"\n",
    "    \"'without pictures or conversations?'\"\n",
    ")\n",
    "\n",
    "def regex_tokenize(text):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_tiktoken(text):\n",
    "    if not TIKTOKEN_AVAILABLE:\n",
    "        # Fallback: character-level bytes-like split so the rest of the notebook remains functional\n",
    "        return list(text)\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    ids = enc.encode(text)\n",
    "    # Return \"tokens\" as string forms of ids for consistent interface\n",
    "    return [str(i) for i in ids]\n",
    "\n",
    "def sliding_windows(tokens, max_length=32, stride=16):\n",
    "    # Build (x, y) pairs where y is x shifted by 1 within each window\n",
    "    # Return list of tuples of ID lists (we'll map strings to ints remapped per tokenizer)\n",
    "    # For simplicity, map each distinct token to an int locally\n",
    "    vocab = {tok: i for i, tok in enumerate(sorted(set(tokens)))}\n",
    "    ids = [vocab[t] for t in tokens]\n",
    "    samples = []\n",
    "    for start in range(0, max(0, len(ids)-max_length), stride):\n",
    "        x = ids[start:start+max_length]\n",
    "        y = ids[start+1:start+max_length+1]\n",
    "        if len(y) == len(x):\n",
    "            samples.append((x, y))\n",
    "    return samples\n",
    "\n",
    "# Run both tokenizers\n",
    "toks_regex = regex_tokenize(sample_text)\n",
    "toks_tk = tokenize_tiktoken(sample_text)\n",
    "\n",
    "# Build datasets\n",
    "max_length = 32\n",
    "stride = 16\n",
    "ds_regex = sliding_windows(toks_regex, max_length=max_length, stride=stride)\n",
    "ds_tk = sliding_windows(toks_tk, max_length=max_length, stride=stride)\n",
    "\n",
    "# Compare\n",
    "def describe(ds, name):\n",
    "    lens = [len(x) for (x, y) in ds]\n",
    "    print(f\"{name}: samples={len(ds)}, avg_seq_len={np.mean(lens) if lens else 0:.2f}\")\n",
    "describe(ds_regex, \"Regex\")\n",
    "describe(ds_tk, \"tiktoken (or fallback)\")\n",
    "\n",
    "# Simple bar plot of sample counts\n",
    "counts = [len(ds_regex), len(ds_tk)]\n",
    "labels = [\"Regex\", \"tiktoken/fallback\"]\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(labels, counts)\n",
    "plt.title(\"Number of samples by tokenizer\")\n",
    "plt.ylabel(\"samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873264f2",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion:**  \n",
    "GPT‑2’s tokenizer (when available) generally yields **shorter sequences** (fewer tokens for the same text) than character-level or naive regex schemes, which often means **fewer windows** for a fixed `max_length`. Conversely, very coarse tokenization may inflate token counts. Real outcomes depend on your text and `max_length`/`stride`.\n",
    "\n",
    "**Vocabulary learned:** *sequence length, dataloader, context window, tokenizer choice impacts batching*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de6003",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Stride experiment — fix `max_length`, vary stride ∈ {1, 8, 64}\n",
    "**Goal:** See how stride changes the number of samples and how much adjacent batches overlap.\n",
    "\n",
    "**What you do:** For each stride, count samples and visualize the first two windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3674744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reuse regex tokenizer for a longer synthetic text\n",
    "long_text = \" \".join([\"This is a tiny sliding window experiment with overlapping sequences.\"] * 20)\n",
    "tokens = re.findall(r\"\\w+|[^\\w\\s]\", long_text)\n",
    "\n",
    "def count_samples(tokens, max_length, stride):\n",
    "    vocab = {tok: i for i, tok in enumerate(sorted(set(tokens)))}\n",
    "    ids = [vocab[t] for t in tokens]\n",
    "    n = 0\n",
    "    starts = []\n",
    "    for start in range(0, max(0, len(ids)-max_length), stride):\n",
    "        if start+max_length < len(ids):\n",
    "            n += 1\n",
    "            starts.append(start)\n",
    "    return n, ids, starts\n",
    "\n",
    "max_length = 24\n",
    "strides = [1, 8, 64]\n",
    "results = {}\n",
    "for s in strides:\n",
    "    n, ids, starts = count_samples(tokens, max_length, s)\n",
    "    results[s] = (n, ids, starts)\n",
    "    print(f\"stride={s}: samples={n}\")\n",
    "\n",
    "# Plot sample counts\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar([str(s) for s in strides], [results[s][0] for s in strides])\n",
    "plt.title(\"Samples vs. stride (fixed max_length)\")\n",
    "plt.xlabel(\"stride\")\n",
    "plt.ylabel(\"samples\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize first 2 windows for each stride\n",
    "def show_windows(ids, starts, max_length, title):\n",
    "    plt.figure(figsize=(10, 1.8))\n",
    "    # draw tokens as ticks\n",
    "    for i in range(len(ids)):\n",
    "        plt.plot([i, i], [0.1, 0.2])\n",
    "    if len(starts) >= 2:\n",
    "        a, b = starts[0], starts[1]\n",
    "    elif len(starts) == 1:\n",
    "        a, b = starts[0], starts[0]\n",
    "    else:\n",
    "        a, b = 0, 0\n",
    "    # Draw two rectangles for window 1 & 2\n",
    "    plt.gca().add_patch(plt.Rectangle((a, 0.3), max_length, 0.4, fill=True, alpha=0.3))\n",
    "    plt.gca().add_patch(plt.Rectangle((b, 0.3), max_length, 0.4, fill=True, alpha=0.3))\n",
    "    plt.title(title)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"token index\")\n",
    "    plt.show()\n",
    "\n",
    "for s in strides:\n",
    "    n, ids, starts = results[s]\n",
    "    show_windows(ids, starts, max_length, f\"First two windows (stride={s})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a226b",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion:**  \n",
    "- **Small stride** → many overlapping windows → more training samples, but higher correlation between adjacent batches.  \n",
    "- **Large stride** → fewer, less-overlapping windows → fewer samples but more diversity between batches.\n",
    "\n",
    "**Vocabulary learned:** *stride, overlap, correlation between batches, data augmentation via overlap*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
